{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.classification import Accuracy, F1\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting our images in the right folders so we can use ImageFolder\n",
    "\n",
    "project_path = os.getcwd().replace('/notebooks', '')\n",
    "csv_path = project_path + '/data/HAM10000_metadata'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    source = project_path + '/data/images/' + row['image_id'] + '.jpg'\n",
    "    destination = project_path + '/data/images/' + row['dx'] + '/' + row['image_id'] + '.jpg'\n",
    "    destination_folder = project_path + '/data/images/' + row['dx']\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.mkdir(destination_folder)\n",
    "    if os.path.exists(source):\n",
    "        shutil.move(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up our dataloaders\n",
    "\n",
    "val_size = 0.1\n",
    "test_size = 0.899\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.CenterCrop((448, 576)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "ds = torchvision.datasets.ImageFolder(project_path + '/data/images2', transform=transform)\n",
    "n_val = int(val_size * len(ds))\n",
    "n_test = int(test_size * len(ds))\n",
    "n_train = len(ds) - (n_val + n_test)\n",
    "train_ds, val_ds, test_ds = random_split(ds, (n_train, n_val, n_test))\n",
    "\n",
    "class_weights = [30.6, 19.5, 9.1, 87.1, 9.0, 1.5, 70.5]\n",
    "train_ds_weights = [class_weights[label] for image, label in train_ds]\n",
    "train_loader = DataLoader(train_ds, batch_size=11, num_workers=6, shuffle=True)\n",
    "weighted_train_loader = DataLoader(train_ds, batch_size=11, num_workers=6, sampler=WeightedRandomSampler(train_ds_weights, len(train_ds), replacement=True))\n",
    "val_loader = DataLoader(val_ds, batch_size=11, num_workers=6, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=11, num_workers=6, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture specification\n",
    "\n",
    "class Baseline(pl.LightningModule):\n",
    "    def __init__(self, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.accuracy = Accuracy()\n",
    "        self.f1 = F1()\n",
    "\n",
    "        self.NN = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*18*14, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "        self.NN = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(448*576*3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.NN(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "        return {\n",
    "           'optimizer': optimizer,\n",
    "           'lr_scheduler': scheduler,\n",
    "           'monitor': 'val_loss'\n",
    "        }\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y_true = train_batch\n",
    "        y_pred = self.NN(x)\n",
    "        loss = F.cross_entropy(y_pred, y_true)\n",
    "        self.log('train_loss', loss, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y_true = val_batch\n",
    "        y_pred = self.NN(x)\n",
    "        loss = F.cross_entropy(y_pred, y_true.view(-1))\n",
    "        acc = self.accuracy(torch.argmax(y_pred, dim=1), y_true.view(-1))\n",
    "        f1 = self.f1(torch.argmax(y_pred, dim=1), y_true.view(-1))\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('accuracy', acc, prog_bar=True)\n",
    "        self.log('f1', f1, prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "# Init model and trainer\n",
    "\n",
    "logger = CSVLogger(\"logs\", name=\"Baseline\", version='test')\n",
    "callbacks = [\n",
    "    #EarlyStopping('val_loss', patience=5),\n",
    "    LearningRateMonitor(logging_interval='step'),\n",
    "    ]\n",
    "trainer = pl.Trainer(gpus=1, auto_lr_find=False, max_epochs=25, logger=logger, callbacks=callbacks)\n",
    "model = Baseline(lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | accuracy | Accuracy   | 0     \n",
      "1 | f1       | F1         | 0     \n",
      "2 | NN       | Sequential | 49.5 M\n",
      "----------------------------------------\n",
      "49.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "49.5 M    Total params\n",
      "198.190   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 22/22 [00:01<00:00, 11.56it/s, loss=0.0558, v_num=test, val_loss=1.390, accuracy=0.362, f1=0.362]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "debug_ds_weights = [1.0 if i==0 or i==1 else 0.0 for i in range(len(train_ds))]\n",
    "debug_loader = DataLoader(train_ds, batch_size=10, num_workers=6, sampler=WeightedRandomSampler(debug_ds_weights, len(train_ds), replacement=True))\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([[-14.0128, -11.5195]])\n",
      "tensor([[ -8.7851, -11.5739]])\n",
      "tensor([[-16.5549, -12.9116]])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds))\n",
    "print(train_ds[0][1])\n",
    "print(train_ds[1][1])\n",
    "print(train_ds[2][1])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(model(train_ds[0][0].unsqueeze(0)))\n",
    "    print(model(train_ds[1][0].unsqueeze(0)))\n",
    "    print(model(train_ds[2][0].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD4CAYAAABPLjVeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUzUlEQVR4nO3de5SV1X3G8e8zwyAIRKIYL4DihZiYBGui5kKMqI0ixhBba0SDl2omSb1AmlramKVtc2ldWbFiUkMnitQsxaRKvUWjxpigVVwCEhFGEVFhYATBLG9oYM759Y8zJQecOZdwZp/Dy/Nx7eWceffZ73YtfNaP/e73fRURmJlZGk31noCZ2c7EoWtmlpBD18wsIYeumVlCDl0zs4T69fUJ3ph6irdH2Ls07zOs3lOwBrTrtBu0vWNsXr+i4sxpGXbgdp+vWq50zcwScuiaWbbkc5W3EiSNlPSQpHZJSyRNKdH3SEk5SaeVm16fLy+YmSWV66rVSF3ANyJioaQhwAJJD0TE0uJOkpqBK4H7KhnUla6ZZUpEvuJWepzojIiF3T+/AbQDw3voejFwG7Cukvk5dM0sW/L5ipukVknzi1prT0NKGgUcDjy+ze+HA6cCMyqdnpcXzCxbylSwW3WNaAPaSvWRNJhCJTs1Il7f5vDVwLSIyEmVbYRw6JpZtpS5QFYNSS0UAvemiJjTQ5cjgFu6A3cYMEFSV0Tc3tuYDl0zy5YqKt1SVEjS64H2iLiqx1NFHFDUfxZwd6nABYeumWVM1G73wlhgMrBY0qLu330T2A8gIipexy3m0DWzbMnXptKNiEeAiu9Yi4hzK+nn0DWzbKnR8kJfceiaWbbU8EJaX3Domlm2uNI1M0uodhfS+oRD18yypUYX0vqKQ9fMMiXCa7pmZul4TdfMLCEvL5iZJeRK18wsodzmes+gJIeumWWLlxfMzBLy8oKZWUKudM3MEnLompmlEw1+Ic0vpjSzbIl85a0ESSMlPSSpXdISSVN66HOWpKe626OSDis3PVe6ZpYttVte6AK+ERELJQ0BFkh6ICKWFvV5ATgmIn4v6SQKL7n8eKlBHbpmli012r0QEZ1AZ/fPb0hqB4YDS4v6PFr0lXnAiHLjOnTNLFv64EKapFHA4cDjJbqdD9xbbiyHrpllSxWVrqRWoLXoV20R0bZNn8EUXsM+NSJe72WcYymE7qfLndOha2bZ0lX5Q8y7A7att+OSWigE7k0RMaeXPmOA64CTImJDuXM6dM0sW2q0pitJwPVAe0Rc1Uuf/YA5wOSIWFbJuA5dM8uW2q3pjgUmA4slLer+3TeB/QAiYgZwObAHcG0ho+mKiCNKDerQNbNsqd3uhUcAlelzAXBBNeM6dM0sW3wbsJlZQn7KmJlZQlXsXqgHh66ZZUtEvWdQkkPXzLLFa7pmZgk5dM3MEvKFNDOzhHK5es+gJIeumWWLlxfMzBJy6JqZJeQ1XTOzdCLvfbpmZul4ecHMLCHvXjAzS8iVrplZQg7dncuASZfQfOiRxJuvsfHKi97dYeAgBkyaQtOwvWHzZt6ZPZ38yyvTT9TqTkN2p//JF6DBu0EEXYt+S9eCB+o9rR1fjR54I2kkcCOwN5Cn8NLK6dv0ETAdmABsBM6NiIWlxnXo1tjmxx9k08O/YMBZX+/x+C6fPZ386hW8M/N7NL1vBLuc9lXevvZbiWdpjSDyOTY99DNi7UvQfwADzrmC3ItLiA1r6j21HVvtKt0u4BsRsVDSEGCBpAciYmlRn5OA0d3t48CPu//dq6Zazc4KciuWEBvf6PV4014jyS17CoD8ug6adn8fGjw00eysobz1WiFwATa9Q35DJxoytK5TyoR8VN5KiIjO/69aI+INoB0Yvk23icCNUTAPGCppn1Lj/smhK+m8P/W7O7Pcmhfod9gnAWjabzR67/vQ0D3qPCurN71nD5r22o/8mhX1nsqOL5eruElqlTS/qLX2NKSkUcDhwOPbHBoOrCr63MG7g3kr21Pp/nNvB4r/Q25Y/NJ2nCJ7Nv3qVjRwMLteOp3+R59CfvUKyDf2FhfrYy27sMupF7H5wdmw6Z16z2aHF/l85S2iLSKOKGpt244naTBwGzA1Il7f9nBPUyg1v5JrupKe6u0QsFdv3+ueeBvAG1NPaezbQ1L7w9u8M/uPa/GDLr+O/Ia1dZyQ1VVTM7ucehFdSx8jt2xBvWeTDTW8I01SC4XAvSki5vTQpQMYWfR5BFByUb7chbS9gBOB3287F+DRMt+1ngwcBJv+ALkuWj5xArnnl8Af3q73rKxO+p90HvkNa+h64v56TyU7avTshe6dCdcD7RFxVS/d7gQuknQLhQtor0VEZ6lxy4Xu3cDgiFjUw4R+U27SO6MBZ/8dzQd9BA1+D4P+6QY23XszNDcDsPnRX9K01wgGnvW3RD5P/uWVvHPLNXWesdVL0/DR9PvwWPLrVtF8bmG1btPc28iv6O0vmFaR2lW6Y4HJwGJJi7p/901gP4CImAHcQ2G72HIKW8bKXutS9PFL3Ly8YD1p3mdYvadgDWjXaTf0tEZalbcuP6PizBn0L7ds9/mq5X26ZpYtfrSjmVlCfrSjmVk64WcvmJkl5ErXzCwhh66ZWUJ+iLmZWTp+R5qZWUoOXTOzhLx7wcwsIVe6ZmYJOXTNzNKJnJcXzMzScaVrZpaOt4yZmaXk0DUzS6ixl3T9CnYzy5boylfcypE0U9I6SU/3cnw3SXdJ+p2kJZW8Jd2ha2bZkq+ilTcLGF/i+IXA0og4DBgH/EBS/1IDennBzDKllhfSImKupFGlugBDul9iORh4FegqNaYrXTPLlioqXUmtkuYXtdYqz/Yj4IMUXru+GJgSUfp9Qa50zSxTqql0I6INaNuO050ILAKOAw4CHpD0cES83tsXXOmaWbbUdk23nPOAOVGwHHgB+ECpLzh0zSxToqvyVgMrgeMBJO0FHAKsKPUFLy+YWabU8g3skmZT2JUwTFIHcAXQAhARM4BvA7MkLQYETIuI9aXGdOiaWbbUMHQjYlKZ42uAE6oZ06FrZplSy0q3Lzh0zSxTHLpmZglFTvWeQkkOXTPLFFe6ZmYJRd6VrplZMq50zcwSinCla2aWjCtdM7OE8t69YGaWji+kmZkl5NA1M0soGvtlwA5dM8sWV7pmZgl5y5iZWUK5Bt+94DdHmFmmRKjiVo6kmZLWSXq6RJ9xkhZJWiLpt+XGdOiaWaZEXhW3CswCxvd2UNJQ4Frg8xHxIeCvyg3o5QUzy5Ra7l6IiLmSRpXociaFF1Ou7O6/rtyYrnTNLFOqqXQltUqaX9Raqzzd+4H3SvqNpAWSzi73BVe6ZpYpuXzltWREtAFt23G6fsDHKLwReCDwmKR5EbGs1BfMzDIj8c0RHcD6iHgLeEvSXOAwoNfQ9fKCmWVKPlRxq4E7gKMl9ZO0K/BxoL3UF1zpmlmm1PLmCEmzgXHAMEkdwBVAS+E8MSMi2iX9EniKwsvfr4uIXreXgUPXzDKmxrsXJlXQ5/vA9ysds89Dd8C3pvf1KWwHNHDfo+s9BWtAXdO2f4waLRv0GVe6ZpYp1exeqAeHrpllSoM/2dGha2bZ4uUFM7OE/GhHM7OEGvxlwA5dM8uWwJWumVkyXV5eMDNLx5WumVlCXtM1M0vIla6ZWUKudM3MEsq50jUzS6ey903Wj0PXzDIl70rXzCydRn/gTWM/A83MrEr5Klo5kmZKWiep5NsgJB0pKSfptHJjOnTNLFPyUsWtArOA8aU6SGoGrgTuq2RAh66ZZUquilZORMwFXi3T7WLgNmBdJfNz6JpZpuRVeZPUKml+UWut5lyShgOnAjMq/Y4vpJlZplSzeyEi2oC27Tjd1cC0iMipsuUKh66ZZUvi3QtHALd0B+4wYIKkroi4vbcvOHTNLFNS3hwREQf8/8+SZgF3lwpccOiaWcbU8tkLkmYD44BhkjqAK4AWgIioeB23mEPXzDIlV8NKNyImVdH33Er6OXTNLFP8lDEzs4QcumZmCTX4K9IcumaWLa50zcwSquT23npy6JpZpvgh5mZmCXl5wcwsIYeumVlCjf7mCIeumWWK13TNzBLy7gUzs4TyDb7A4NA1s0zxhTQzs4Qau8516JpZxjR6pesXU5pZpnQpKm7lSJopaZ2kp3s5fpakp7rbo5IOKzemQ9fMMiWqaBWYBYwvcfwF4JiIGAN8mwpecunlBTPLlFouL0TEXEmjShx/tOjjPGBEuTFd6ZpZpuSJipukVknzi1rrdpz6fODecp1c6ZpZplSzeyEi2qhgSaAcScdSCN1Pl+vr0DWzTEm9e0HSGOA64KSI2FCuv0PXzDIll3CnrqT9gDnA5IhYVsl3HLpmlim1rHQlzQbGAcMkdQBXAC0AETEDuBzYA7hWEkBXRBxRakyHrpllStSw0o2ISWWOXwBcUM2YDl0zyxTfkbaT6Vz7CuddNI1Tzmxl4llf4ac/v73Xvovbn2XM0Sdz/0MPp5ugJXXiCeNY8vRcnln6CH9/6YU99jnmM59k/hP387tFv+bXv7oVgPe//yDmP3H/lvbq+me45OKqCqqdVjVbxurBlW6N9Wtu5tKLv8yhhxzMW29t5PTzL+FTRx7OQQfsv1W/XC7Hv197A2OP+midZmp9rampiWumf5fxEybR0dHJvMfu4a6776e9/bktfXbb7T388Iff4+TPncWqVWvYc889AFi27HmOOPKELeOsfHEBt99Rdguo0fgPvHGlW2N7DtudQw85GIBBg3blwP1HsvaVd+8iufnWO/nsuLHs/t6hiWdoqRx15OE8//yLvPDCSjZv3szPf34Hnz/lxK36TDrjVG6//V5WrVoDwCs9/Fk5/rhPs2LFS6xcuTrJvHd0XUTFrR7Khq6kD0g6XtLgbX5f6n5kA1Z3rqX9uecZ86FDtvr92lfW8+DcRzn9CxPqNDNLYd/he7OqY82Wzx2rO9l337236jN69IEMHbobDz7w3zw+716+9KXT3jXO6adP5Jaf3d7X082MqOKfeigZupIuAe4ALgaeljSx6PD3Snxvy6111904uzYz3cFs3Pg2X7/sO0y75CsMHjRoq2NXTv9Pvv61v6a5ublOs7MUurcQbSVi6//R+/Vr5mMfHcMpE89mwslnctk/TmX06AO3HG9paeGUz53Arbfd3efzzYp8Fa0eyq3pfhn4WES82f3Qh1sljYqI6UCvr38rvrVu8/oVjb7EUnObu7qYetl3OPmEY/nsuLHvOr7kmee49Ip/A+D3r73Ow489QXNzM8d/5lOpp2p9aHVHJyNH7Lvl84jh+9DZuXbrPqs72bDhVTZufJuNG9/m4UfmMWbMoTz33AoAxo8/liefXMy6deuTzn1HVq8KtlLlQrc5It4EiIgXJY2jELz7UyJ0d2YRweX/ejUH7j+Sc874ix773HfrrC0/X/adH3DM2KMcuBn0xPxFHHzwAYwaNZLVq1/m9NMnMvnsrXcw3HnXfVxz9Xdpbm6mf/8WjjrqcKZf85Mtx8/44he8tFClRt8yVi50X5b0ZxGxCKC74v0cMBP4SF9Pbkf05FNLuOuXDzL6oFH85TmF/8GmfOUcOte+AsAXTz25ntOzhHK5HFOmfot7fnEzzU1NzPqvn7F06TJavzwZgLaf/JRnnlnOffc/xJMLf0U+n2fmzNksWfIsAAMHDuDPj/8MX/ubafX8z9jh5KKxK11tu8a01UFpBIXb2l7u4djYiPjfcifYGZcXrLyB+x5d7ylYA+ratHq7/wZ95v6nVpw5N7/0P8n/xl6y0o2IjhLHygaumVlqO/qarpnZDmVHX9M1M9uh1Ov23ko5dM0sU7y8YGaWUKPvXnDomlmmNPrygh94Y2aZUsvbgCXNlLRO0tO9HJekayQtl/SUpLKPDXTomlmm1PiBN7OAUg/3OgkY3d1agR+XG9Cha2aZUsuHmEfEXODVEl0mAjdGwTxgqKR9So3p0DWzTImIilvxExG7W2uVpxsOrCr63NH9u175QpqZZUo1r2AvfiLin6in24hLTsCha2aZknj3QgcwsujzCGBNL30BLy+YWcZUs7xQA3cCZ3fvYvgE8FpEdJb6gitdM8uUWla6kmYD44BhkjqAK4AWgIiYAdwDTACWAxuB88qN6dA1s0yp5W3AETGpzPEALizVZ1sOXTPLFN8GbGaWUKPfBuzQNbNMceiamSVUo10Jfcaha2aZ4krXzCwhP8TczCyhXDT2W9IcumaWKV7TNTNLyGu6ZmYJeU3XzCyhvJcXzMzScaVrZpaQdy+YmSXk5QUzs4QafXnBb44ws0zJR1TcypE0XtKzkpZL+oceju8m6S5Jv5O0RFLZh5g7dM0sU6KKf0qR1Az8B3AScCgwSdKh23S7EFgaEYdReMPEDyT1LzWulxfMLFNykavVUEcByyNiBYCkW4CJwNKiPgEMkSRgMPAq0FVqUFe6ZpYp1byYUlKrpPlFrbVoqOHAqqLPHd2/K/Yj4IMU3gC8GJgSUXr7hCtdM8uUam4Djog2oK2Xw+rpK9t8PhFYBBwHHAQ8IOnhiHi9t3O60jWzTKnhK9g7gJFFn0dQqGiLnQfMiYLlwAvAB0oN6tA1s0yp4e6FJ4DRkg7ovjh2BnDnNn1WAscDSNoLOARYUWpQLy+YWabUap9uRHRJugi4D2gGZkbEEklf7T4+A/g2MEvSYgrLEdMiYn2pcdXXz57cvH5FY+9UtroYuO/R9Z6CNaCuTat7Wketyp67HVJx5rzy2rPbfb5qudI1s0zxQ8zNzBLysxfMzBJypWtmlpBf12NmlpArXTOzhPwQczOzhHwhzcwsIS8vmJkl1OhvjnDomlmmuNI1M0uo0dd0+/zZC/ZHklq7n99ptoX/XOxc/GjHtFrLd7GdkP9c7EQcumZmCTl0zcwScuim5XU764n/XOxEfCHNzCwhV7pmZgk5dM3MEnLoJiBppqR1kp6u91yscUgaKekhSe2SlkiaUu85Wd/zmm4Ckj4DvAncGBEfrvd8rDFI2gfYJyIWShoCLAC+EBFL6zw160OudBOIiLnAq/WehzWWiOiMiIXdP78BtAPD6zsr62sOXbMGIGkUcDjweJ2nYn3MoWtWZ5IGA7cBUyPi9XrPx/qWQ9esjiS1UAjcmyJiTr3nY33PoWtWJ5IEXA+0R8RV9Z6PpeHQTUDSbOAx4BBJHZLOr/ecrCGMBSYDx0la1N0m1HtS1re8ZczMLCFXumZmCTl0zcwScuiamSXk0DUzS8iha2aWkEPXzCwhh66ZWUL/B5oyl0OqBA2hAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "val_ds_weights = [class_weights[label] for image, label in val_ds]\n",
    "weighted_val_loader = DataLoader(val_ds, batch_size=25, num_workers=6, sampler=WeightedRandomSampler(val_ds_weights, len(val_ds), replacement=True))\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over validation data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        for inputs, labels in weighted_val_loader:\n",
    "                logits = model(inputs) # Feed Network\n",
    "\n",
    "                output = (torch.max(torch.exp(logits), 1))[1].cpu().numpy()\n",
    "                y_pred.extend(output) # Save Prediction\n",
    "                \n",
    "                labels = labels.data.cpu().numpy()\n",
    "                y_true.extend(labels) # Save Truth\n",
    "\n",
    "# constant for classes\n",
    "classes = ('akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc')\n",
    "classes = [1, 2]\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix)*7, index = [i for i in classes], columns = [i for i in classes])\n",
    "sns.heatmap(df_cm, annot=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd7180015afb5c2b39819f98dde8a9af643fe9afa196f2eb6eaf1e4a6c9ffccd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}